---
title: "extendedglmnet"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(extendedglmnetGroup7)
```

# Introduction

The `extendedglmnetGroup7` package was written by Victoria Hamilton and Scott Nelson as part of a final project for AMS 597 at Stony Brook University in Spring 2022. The primary purpose of the package is to extend the `glmnet` package to inlude random lasso as described in Wang, S., Nan, B., Rosset, S., & Zhu, J. (2011). Random lasso. *The Annals of Applied Statistics*, 5(1), 468-485.

The `extendedglmnetGroup7` package also includes functions for doing basic linear and logistic regression which are not included in the `glmnet` package, as well as functions that by default perform ridge and lasso regression. In total, the `extendedglmnetGroup7` package can fit the following types of models:

- Linear Regression
- Logistic Regression
- Ridge Regression (for binary or continuous response variable)
- Lasso Regression (for binary or continuous response variable)
- Random Lasso (for binary or continuous response variable)

# Linear Regression

Linear regression models a linear relation between a response variable and a set of predictor variables. These types of models take the form: $$y_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip}+\epsilon$$ where $y_i$ is the $i$th response, $\beta_0,\beta_1,\cdots,\beta_p$ is a parameter vector where $\beta_0$ is the intercept and the remaining terms are coefficients for the various predictor variables, $x_{i1},\cdots,x_{ip}$ are observed values for the various parameters of the $i$th data point, and $\epsilon$ is random noise. Given a set of observations and responses, the goal of linear regression is to find the coefficients that minimize an error term (often the sum of squared errors.)

A linear regression model can be fit using the function `extended.lm()`. This function is a wrapper for the base `lm()` function. Unlike the `lm()` function, `extended.lm()` takes two variables and does not require a formula. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ continuous response values. The remainder of the input variables used with `lm()` are kept as their default values. For more information on `lm()` see the [package documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm).


## Example ##

First we create some data. 

```{r}
n <- 1000
B0 <- 3
B1 <- 5
X1 <- rnorm(n)
B2 <- -2
X2 <- rnorm(n)
Y <- B0 + B1*X1 + B2*X2 + rnorm(n,0,10)
X <- as.matrix(cbind(X1=X1,X2=X2))
```

Next, we can fit the model using `extended.lm()` and check to see the predicted coefficients.

```{r}
fit <- extended.lm(X,Y)
coef(fit)
```

As expected, the fitted coefficients are close to the true coefficients `B0`, `B1`, and `B2`.

# Logistic Regression

Logistic regression is a type of generalized linear regression that models binary outcomes. Generalized linear regression models are characterized by a response distribution (binomial) and a link function which relates linear predictors and the mean of the response distribution. $$ E(X) = \mu = g^{-1}(X\beta) $$

In regression, we want to explain a response variable $Y$ as a function of predictors $X$. For logistic regression with a binary response we need to restrict $Y$ to be between 0 and 1. This is accomplished with the following model: $$ E(Y) = \mu = \frac{\exp(X\beta)}{1+\exp(X\beta)} $$

Finally, the linking function is the logit function which is defined as: $$ X\beta = \log(\frac{\mu}{1-\mu}) $$

A logistic regression model can be fit using the function `extended.glm()`. This function is a wrapper for the base `glm()` function. Unlike the `glm()` function, `extended.glm()` takes two variables and does not require a formula. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ binary response values. The function sets `family=binomial("logit")`. The remainder of the input variables used with `glm()` are kept as their default values. For more information on `glm()` see the [package documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm).

## Example ##

We can use the same `X` and `B` values from the data created in the linear regression section. All we need to change is how `Y` is derived.

```{r}
Z <- B0 + B1*X1 + B2*X2
pr=exp(Z)/(1+exp(Z))
Y2 <- rbinom(n,1,pr)
```

Next, we can fit the model using `extended.glm()` and check to see the predicted coefficients.

```{r}
fit <- extended.glm(X,Y2)
coef(fit)
```

As expected, the fitted coefficients are close to the true coefficients `B0`, `B1`, and `B2`.

# Ridge Regression

In linear regression, the coefficients are determined by minimizing the residual sum of squares. That is, it finds a vector $\hat{\beta}$ that minimizes: $$ RSS = \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2 $$
Ridge regression extends this by adding a non-negative tuning parameter $\lambda$ that is added to each non-intercept coefficient in $\hat{\beta}$. Ridge regression therefore minimizes: $$ RSS + \lambda\sum_{j=1}^p\beta_j^2 $$ As $\lambda$ goes to 0, ridge regression becomes normal linear regression. As $\lambda$ goes to positive infinity, the $\hat{\beta}$ coefficients approach 0. The advantage of ridge regression is that it reduces variance, but this is done by slightly increasing *bias$ in the model. 

A ridge regression model can be fit using the function `extended.ridge()`. This function is a wrapper for the function `glmnet::glmnet()` with the argument `alpha=0` hard coded in. The `alpha` parameter for `glmnet::glmnet()` indicates the type of penalty to be used in the model. `0` is the ridge penalty. Like `extended.lm()` and `extended.glm()`, `extended.ridge()` takes separate inputs for the predictors and responses. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ of either binary or continuous response values.

`extended.ridge()` also can take a user provided `lambda` value. By default this variable is set to `NULL`. If a `lambda` value is not provided, `extended.ridge()` automatically determines a value using ``glmnet::cv.glmnet(alpha=0,nfolds=5)`. The final argument to `extended.ridge()` is `ytype`. The possible forms of this argument are `"continuous"` and `"binary"`. The default argument is `continuous`. 

## Example ##

For this example we will use the leukemia gene dataset from [http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt](http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt). This is saved as a part of the `extendedglmGroup7` package as `dat`.

```{r}

```


# Lasso Regression

# Random Lasso Regression



