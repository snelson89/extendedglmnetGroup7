---
title: "extendedglmnet"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(extendedglmnetGroup7)
```

# Introduction

The `extendedglmnetGroup7` package was written by Victoria Hamilton and Scott Nelson as part of a final project for AMS 597 at Stony Brook University in Spring 2022. The primary purpose of the package is to extend the `glmnet` package to inlude random lasso as described in Wang, S., Nan, B., Rosset, S., & Zhu, J. (2011). Random lasso. *The Annals of Applied Statistics*, 5(1), 468-485.

The `extendedglmnetGroup7` package also includes functions for doing basic linear and logistic regression which are not included in the `glmnet` package, as well as functions that by default perform ridge and lasso regression. In total, the `extendedglmnetGroup7` package can fit the following types of models:

- Linear Regression
- Logistic Regression
- Ridge Regression (for binary or continuous response variable)
- Lasso Regression (for binary or continuous response variable)
- Random Lasso (for binary or continuous response variable)

# Linear Regression

Linear regression models a linear relation between a response variable and a set of predictor variables. These types of models take the form: $$y_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip}+\epsilon$$ where $y_i$ is the $i$th response, $\beta_0,\beta_1,\cdots,\beta_p$ is a parameter vector where $\beta_0$ is the intercept and the remaining terms are coefficients for the various predictor variables, $x_{i1},\cdots,x_{ip}$ are observed values for the various parameters of the $i$th data point, and $\epsilon$ is random noise. Given a set of observations and responses, the goal of linear regression is to find the coefficients that minimize an error term (often the sum of squared errors.)

A linear regression model can be fit using the function `extended.lm()`. This function is a wrapper for the base `lm()` function. Unlike the `lm()` function, `extended.lm()` takes two variables and does not require a formula. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ continuous response values. The remainder of the input variables used with `lm()` are kept as their default values. For more information on `lm()` see the [package documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm).


## Example ##

First we create some data. 

```{r}
n <- 1000
B0 <- 3
B1 <- 5
X1 <- rnorm(n)
B2 <- -2
X2 <- rnorm(n)
Y <- B0 + B1*X1 + B2*X2 + rnorm(n,0,10)
X <- as.matrix(cbind(X1=X1,X2=X2))
```

Next, we can fit the model using `extended.lm()` and check to see the predicted coefficients.

```{r}
fit <- extended.lm(X,Y)
coef(fit)
```

As expected, the fitted coefficients are close to the true coefficients `B0`, `B1`, and `B2`.

# Logistic Regression

Logistic regression is a type of generalized linear regression that models binary outcomes. Generalized linear regression models are characterized by a response distribution (binomial) and a link function which relates linear predictors and the mean of the response distribution. $$ E(X) = \mu = g^{-1}(X\beta) $$

In regression, we want to explain a response variable $Y$ as a function of predictors $X$. For logistic regression with a binary response we need to restrict $Y$ to be between 0 and 1. This is accomplished with the following model: $$ E(Y) = \mu = \frac{\exp(X\beta)}{1+\exp(X\beta)} $$

Finally, the linking function is the logit function which is defined as: $$ X\beta = \log(\frac{\mu}{1-\mu}) $$

A logistic regression model can be fit using the function `extended.glm()`. This function is a wrapper for the base `glm()` function. Unlike the `glm()` function, `extended.glm()` takes two variables and does not require a formula. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ binary response values. The function sets `family=binomial("logit")`. The remainder of the input variables used with `glm()` are kept as their default values. For more information on `glm()` see the [package documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm).

## Example ##

We can use the same `X` and `B` values from the data created in the linear regression section. All we need to change is how `Y` is derived.

```{r}
Z <- B0 + B1*X1 + B2*X2
pr=exp(Z)/(1+exp(Z))
Y2 <- rbinom(n,1,pr)
```

Next, we can fit the model using `extended.glm()` and check to see the predicted coefficients.

```{r}
fit <- extended.glm(X,Y2)
coef(fit)
```

As expected, the fitted coefficients are close to the true coefficients `B0`, `B1`, and `B2`.

# Ridge Regression

In linear regression, the coefficients are determined by minimizing the residual sum of squares. That is, it finds a vector $\hat{\beta}$ that minimizes: $$ RSS = \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2 $$
Ridge regression extends this by adding a non-negative tuning parameter $\lambda$ that is added to each non-intercept coefficient in $\hat{\beta}$. Ridge regression therefore minimizes: $$ RSS + \lambda\sum_{j=1}^p\beta_j^2 $$ As $\lambda$ goes to 0, ridge regression becomes normal linear regression. As $\lambda$ goes to positive infinity, the $\hat{\beta}$ coefficients approach 0. The advantage of ridge regression is that it reduces variance, but this is done by slightly increasing *bias$ in the model. 

A ridge regression model can be fit using the function `extended.ridge()`. This function is a wrapper for the function `glmnet::glmnet()` with the argument `alpha=0` hard coded in. The `alpha` parameter for `glmnet::glmnet()` indicates the type of penalty to be used in the model. `0` is the ridge penalty. Like `extended.lm()` and `extended.glm()`, `extended.ridge()` takes separate inputs for the predictors and responses. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ of either binary or continuous response values.

`extended.ridge()` also can take a user provided `lambda` value. By default this variable is set to `NULL`. If a `lambda` value is not provided, `extended.ridge()` automatically determines a value using ``glmnet::cv.glmnet(alpha=0,nfolds=5)`. This is the suggested method for users. The final argument to `extended.ridge()` is `ytype`. This indicates whether or not the response variable is continuous or binary. The possible forms of this argument are `"continuous"` and `"binary"`. The default argument is `continuous`. These control the `family` argument of the `glmnet::glmnet()` function. If `ytype="continuous"` then `family="gaussian"`. If `ytype="binary"` then `family="binomial"`. All other `glmnet::glmnet()` arguments are kept as their default values. For mor information on `glmnet::glmnet()` see the [package documentation](https://www.rdocumentation.org/packages/glmnet/versions/4.1-4/topics/glmnet) or [vignette](https://glmnet.stanford.edu/articles/glmnet.html).

## Example ##

For this example we will use the leukemia gene data set from [http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt](http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt). This is saved as a part of the `extendedglmGroup7` package as `dat`. This data contains 72 binary observations and 3571 predictors. 

```{r}
X <- dat[,2:ncol(dat)]
Y <- dat[,1]

fit <- extended.ridge(X,Y,ytype="binary")
coef(fit)[1:21]
```

We can look at the intercept and the next 20 coefficients and see that they are all small, but nonzero. 

Let's use the function `create.regr.data()` for a second example. We will make the data set the same size as the gene data set above. This time, we will also provide our own lambda value.

```{r}
dd <- create.regr.data(72,3751)$Data
X2 <- dd[,1:(ncol(dd)-1)]
Y2 <- dd[,c(ncol(dd))]

fit <- extended.ridge(X2,Y2,lambda=100,ytype="continuous")
coef(fit)[1:21]
```

We once again look at the intercept and the next 20 coefficients and see that many are small, but nonzero. 

# Lasso Regression

Lasso regression is similar to ridge regression. The only difference is how the penalty term is calculated. In ridge regression, $\lambda$ is multiplied by the square of each $\beta$ coefficient while with lasso regression, $\lambda$ is multiplied by the absolute value of each $\beta$ coefficient during the minimization process. Therefore, lasso regression minimizes: $$RSS + \lambda\sum_{j=1}^p|\beta_j|$$ This causes the coefficients to shrink *exactly* to zero. Lasso regression can therefore be used as a type of predictor selection process. 

A lasso regression model can be fit using the function `extended.lasso()`. This function is a wrapper for the function `glmnet::glmnet()` with the argument `alpha=1` hard coded in. The `alpha` parameter for `glmnet::glmnet()` indicates the type of penalty to be used in the model. `1` is the lasso penalty. Like `extended.lm()` and `extended.glm()`, `extended.lasso()` takes separate inputs for the predictors and responses. `X` is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. `Y` is a vector of length $n$ of either binary or continuous response values.

`extended.lasso()` also can take a user provided `lambda` value. By default this variable is set to `NULL`. If a `lambda` value is not provided, `extended.ridge()` automatically determines a value using ``glmnet::cv.glmnet(alpha=1,nfolds=5)`. This is the suggested method for users. The final argument to `extended.lassoe()` is `ytype`. This indicates whether or not the response variable is continuous or binary. The possible forms of this argument are `"continuous"` and `"binary"`. The default argument is `continuous`. These control the `family` argument of the `glmnet::glmnet()` function. If `ytype="continuous"` then `family="gaussian"`. If `ytype="binary"` then `family="binomial"`. All other `glmnet::glmnet()` arguments are kept as their default values. For mor information on `glmnet::glmnet()` see the [package documentation](https://www.rdocumentation.org/packages/glmnet/versions/4.1-4/topics/glmnet) or [vignette](https://glmnet.stanford.edu/articles/glmnet.html).

## Example ##

For this example we will use the leukemia gene data set from [http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt](http://www.ams.sunysb.edu/~pfkuan/Teaching/AMS597/Data/leukemiaDataSet.txt). This is saved as a part of the `extendedglmGroup7` package as `dat`. This data contains 72 binary observations and 3571 predictors. 

```{r, message=FALSE}
X <- dat[,2:ncol(dat)]
Y <- dat[,1]

fit <- extended.lasso(X,Y,ytype="binary")
nonzero.c <- length(coef(fit)[coef(fit) != 0])
paste("There are",nonzero.c-1,"zonzero coefficients")
```

We can look to see how many coefficients are nonzero. Notice that with this method we *drastically* reduce the number of predictors that affect the outcome. 

Let's use the function `create.regr.data()` for a second example. We will make the data set the same size as the gene data set above. This time, we will also provide our own lambda value.

```{r, message=FALSE}
dd <- create.regr.data(72,3751)$Data
X2 <- dd[,1:(ncol(dd)-1)]
Y2 <- dd[,c(ncol(dd))]

fit <- extended.lasso(X2,Y2,lambda=100,ytype="continuous")
nonzero.c <- length(coef(fit)[coef(fit) != 0])
paste("There are",nonzero.c-1,"zonzero coefficients")
```

We once again see that with this method we *drastically* reduce the number of predictors that affect the outcome.. 

# Random Lasso Regression

Random lasso is an extension of lasso regression. With random lasso, there are two steps. The first step performs lasso regression on a randomly selected group of predictors of Bootstrap samples. This is done to create an importance measure for each predictor. In step two, lasso regression is once again performed on a randomly selected group of predictors, but this time the probability of a predictor being chosen is weighted by its importance measure in step one.

`extendedglmnetGroup7` has a custom implementation of random lasso regression based on the algorithm in [Wang et al. (2012)](https://www.ncbi.nlm.nih.gov/pmc/articles/pmc3445423/). The implementation of the algorithm uses `glmnet::glmnet(alpha=1)` for the lasso regression. The algorithm assumes numerical predictors that have been mean centered and therefore fits an intercept-free model. By default `glmnet::glmnet()` centers the data with the argument `standardize=TRUE`. An extra argument `intercept=FALSE` was included in the implementation of the algortihm to ensure that it fit an intercept-free model.



